{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "54hsYo44eQ1Y"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhfring/parallel-c/blob/main/TSP_Cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "1.   **GPU Runtime**: click on the \"Runtime\" menu item in the top bar and select the \"Change runtime type\" option. Select \"GPU\" from the list of Hardware accelerators and click \"Ok\".  \n",
        "\n",
        "2.   CUDA Compilation: we will use of the NVCC4Jupyter plugin which effectively turns any Colab Notebook code block that includes `%%cu` into compilable/runnable CUDA code."
      ],
      "metadata": {
        "id": "O7RFj9ykaSzY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYXTE96HMb_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3deb3fa8-c981-405b-d347-0dedc575832d"
      },
      "source": [
        "# first run this to install and load nvcc plugin \n",
        "!pip install git+https://github.com/engasa/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/engasa/nvcc4jupyter.git\n",
            "  Cloning https://github.com/engasa/nvcc4jupyter.git to /tmp/pip-req-build-7u468430\n",
            "  Running command git clone -q https://github.com/engasa/nvcc4jupyter.git /tmp/pip-req-build-7u468430\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4406 sha256=e599efc6cfe854b50d3cd4abfec771526d92af64a58f06518c2d9e1cfadfa8f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e1u6kaow/wheels/36/86/36/c7b00095a61c28f9bf69a386c706b14b45c600ce89dc6c16b2\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   Now you can check your CUDA installation by running the command below. The output should show you some info about the Cuda compiler, e.g., \"*nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021* ...etc\""
      ],
      "metadata": {
        "id": "7MwvWlydb9W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check nvcc version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "I1ZHiWr-cNJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeee9f06-a1dc-43e5-d4c3-f435638570ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   You can also check if GPU has been allocated. Colab notebooks without a GPU technically have access to NVCC and will compile and execute CPU/Host code, however, GPU/Device code will silently fail. To prevent such situations, this code will warn the user.\n"
      ],
      "metadata": {
        "id": "x-dAfZOCiayz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "int main() {\n",
        "    int count;\n",
        "    cudaGetDeviceCount(&count);\n",
        "    if (count <= 0 || count > 100)  printf(\"!!!!! WARNING<-: NO GPU DETECTED ON THIS COLLABORATE INSTANCE. YOU SHOULD CHANGE THE RUNTIME TYPE.!!!!!\\n\");\n",
        "    else                            printf(\"^^^^ GPU ENABLED! ^^^^\\n\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "W1IWUSIFiSkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21066d37-352c-42f1-a521-6cd13dd9a3ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^^^^ GPU ENABLED! ^^^^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <stdint.h>\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#define MAX_THREADS 1024\n",
        "#define MAX_BLOCKS 30\n",
        "#define MAX_PERMS 5041\n",
        "\n",
        "#define CUDA_RUN(x_) {cudaError_t cudaStatus = x_; if (cudaStatus != cudaSuccess) {fprintf(stderr, \"Error  %d - %s\\n\", cudaStatus, cudaGetErrorString(cudaStatus)); goto Error;}}\n",
        "#define SAFE(x_) {if((x_) == NULL) printf(\"out of memory. %d\\n\", __LINE__);}\n",
        "\n",
        "\n",
        "\n",
        "//For host functions\n",
        "__host__ unsigned long long factorial(int32_t n);\n",
        "//_host__ void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size);\n",
        "\n",
        "__host__ unsigned long long factorial(int32_t n) {\n",
        "\tint c;\n",
        "\tunsigned long long result = 1;\n",
        "\n",
        "\tfor (c = 1; c <= n; c++){\n",
        "\t\tresult = result * c;\n",
        "  }\n",
        "\n",
        "\treturn result;\n",
        "}\n",
        "\n",
        "__host__\n",
        "void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size) {\n",
        "  printf(\"Initializing the problem\\n\");\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tcity_ids[i] = i;\n",
        "\t\tfor (int j = 0; j < size; j++) {\n",
        "\t\t\tif (i == j)\n",
        "\t\t\t\tgraphWeights[i * size + j] = 0;\n",
        "\t\t\telse\n",
        "\t\t\t\tgraphWeights[i * size + j] = 99;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tfor (int j = 0; j < size;) {\n",
        "\t\t\tint next = 1; // (rand() % 2) + 1;\n",
        "\t\t\tint road = rand() % 100 + 1;\n",
        "\t\t\tif (i == j) {\n",
        "\t\t\t\tj += next;\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\t}\n",
        "\t\t\tgraphWeights[i * size + j] = road;\n",
        "\t\t\tj += next;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = size - 1; i >= 0; i--) {\n",
        "\t\tgraphWeights[((i + 1) % size) * size + i] = 1;\n",
        "\t}\n",
        "  for(int i=0; i<size; i++){\n",
        "      for(int j=0; j<size; j++){\n",
        "          printf(\"%d,\\t\", graphWeights[i * size + j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "void print_Graph(int8_t * graphWeights, int32_t size) {\n",
        "\tint i, j;\n",
        "\tfor (i = 0; i < size; i++) {\n",
        "\t\tfor (j = 0; j < size; j++) {\n",
        "\t\t\tprintf(\"%d\\t\", graphWeights[i * size + j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "}\n",
        "\n",
        "__host__\n",
        "void print_ShortestPath(int8_t * shortestPath, int32_t cost, int32_t size) {\n",
        "\tint i;\n",
        "\tif (cost == (size * 100)) printf(\"no possible path found.\\n\");\n",
        "\telse {\n",
        "\t\tfor (i = 0; i < size; i++) {\n",
        "\t\t\tprintf(\"%d\\t\", shortestPath[i]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\nCost: %d\\n\", cost);\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "//For device functions\n",
        "__device__ __shared__ int32_t shared_cost;\n",
        "\n",
        "__device__\n",
        "void coppy_array(int8_t * path, int8_t * shortestPath, int32_t * tcost, int8_t * weights, int8_t length, int tid) {\n",
        "\tint32_t sum = 0;\n",
        "\tfor (int32_t i = 0; i < length; i++) {\n",
        "\t\tint8_t val = weights[path[i] * length + path[(i + 1) % length]];\n",
        "\t\tif (val == -1) return;\n",
        "\t\tsum += val;\n",
        "\t}\n",
        "\tif (sum == 0) return;\n",
        "\tatomicMin(&shared_cost, sum);\n",
        "\tif (shared_cost == sum) {\n",
        "\t\t*tcost = sum;\n",
        "\t\tmemcpy(shortestPath, path, length * sizeof(int32_t));\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__\n",
        "void swap(int8_t *x, int8_t *y) { int8_t tmp = *x; *x = *y;\t*y = tmp; }\n",
        "\n",
        "__device__\n",
        "void reverse(int8_t *first, int8_t *last) { while ((first != last) && (first != --last)) swap(first++, last); }\n",
        "\n",
        "\n",
        "__device__\n",
        "bool next_permutation(int8_t * first, int8_t * last) {\n",
        "\tif (first == last) return false;\n",
        "\tint8_t * i = first;\n",
        "\t++i;\n",
        "\tif (i == last) return false;\n",
        "\ti = last;\n",
        "\t--i;\n",
        "\n",
        "\tfor (;;) {\n",
        "\t\tint8_t * ii = i--;\n",
        "\t\tif (*i < *ii) {\n",
        "\t\t\tint8_t * j = last;\n",
        "\t\t\twhile (!(*i < *--j));\n",
        "\t\t\tswap(i, j);\n",
        "\t\t\treverse(ii, last);\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\t\tif (i == first) {\n",
        "\t\t\treverse(first, last);\n",
        "\t\t\treturn false;\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__\n",
        "void find_permutations_for_threads(int8_t * city_ids, int8_t * k, int8_t * choices, int32_t * size, unsigned long long * threads_per_kernel) {\n",
        "  printf(\"In the kernel\");\n",
        "  int32_t length = *size;\n",
        "\tint8_t index = 1;\n",
        "\tunsigned long long count = 0;\n",
        "\tfor (count = 0; count < *threads_per_kernel; count++) {\n",
        "\t\tfor (int i = 0; i < length; i++) {\n",
        "\t\t\tchoices[i + count * length] = city_ids[i];\n",
        "\t\t}\n",
        "\t\treverse(city_ids + *k + index, city_ids + length);\n",
        "\t\tnext_permutation(city_ids + index, city_ids + length);\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "__global__\n",
        "void combinations_kernel(int8_t * choices, int8_t * k, int8_t * shortestPath, int8_t * graphWeights, int32_t * cost, int32_t * size) {\n",
        "    \n",
        "\tuint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint32_t length = *size;\n",
        "\tint8_t index = 1;\n",
        "\n",
        "\t/* local variables */\n",
        "\tint8_t * _path, *_shortestPath;\n",
        "\tint32_t _tcost;\n",
        "\n",
        "\tSAFE(_path = (int8_t *)malloc(length * sizeof(int8_t)));\n",
        "\tSAFE(_shortestPath = (int8_t *)malloc(length * sizeof(int8_t)));\n",
        "\t_tcost = length * 100;\n",
        "\n",
        "\tmemcpy(_path, choices + tid * length, length * sizeof(int8_t));\n",
        "\tmemcpy(_shortestPath, shortestPath, length * sizeof(int8_t));\n",
        "\n",
        "\tif (threadIdx.x == 0) {\n",
        "\t\tif (cost[blockIdx.x] == 0) cost[blockIdx.x] = length * 100;\n",
        "\t\tshared_cost = length * 100;\n",
        "\t}\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\tdo {\n",
        "\t\tcoppy_array(_path, _shortestPath, &_tcost, graphWeights, length, tid);\n",
        "\t} while (next_permutation(_path + *k + index, _path + length));\n",
        "\n",
        "\tif (_tcost == shared_cost) {\n",
        "\t\tatomicMin(&cost[blockIdx.x], _tcost);\n",
        "\t\tif (cost[blockIdx.x] == _tcost) {\n",
        "\t\t\tmemcpy(shortestPath + blockIdx.x * length, _shortestPath, length * sizeof(int8_t));\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfree(_path);\n",
        "\tfree(_shortestPath);\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "\tint size8 = sizeof(int8_t);\n",
        "\tint size32 = sizeof(int32_t);\n",
        "\tunsigned long long total_permutations, thread_perms, num_blocks = 1, num_threads, num_kernels = 1;\n",
        "\tfloat time_passed;\n",
        "\tcudaEvent_t startEvent, stopEvent;\n",
        "\t/* host variables */\n",
        "\tint8_t * city_ids, *shortestPath, *graphWeights, *choices;\n",
        "  int32_t size = 5, *cost;\n",
        "\tint8_t selected_K = 0;\n",
        "\tunsigned long long threads_per_kernel;\n",
        "\t/* device variables */\n",
        "\tint8_t * dev_city_ids, *dev_shortestPath, *dev_graphWeights, *dev_choices;\n",
        "\tint32_t * dev_cost, *dev_size;\n",
        "\tint8_t * dev_selected_K;\n",
        "\tunsigned long long * dev_threads_per_kernel;\n",
        "\n",
        "\ttotal_permutations = factorial(size - 1);\n",
        "\tprintf(\"factorial(%d): %llu\\n\", size - 1, total_permutations);\n",
        "\n",
        "\tfor (selected_K = 1; selected_K < size - 2; selected_K++) {\n",
        "\t\tthread_perms = factorial(size - 1 - selected_K);\n",
        "\t\tif (thread_perms < MAX_PERMS) break;\n",
        "\t}\n",
        "  \n",
        "\tnum_threads = total_permutations / thread_perms;\n",
        "\tint k;\n",
        "\twhile (num_threads > MAX_THREADS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_threads % k != 0) k++;\n",
        "\t\tnum_threads /= k;\n",
        "\t\tnum_blocks *= k;\n",
        "\t}\n",
        "\twhile (num_blocks > MAX_BLOCKS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_blocks % k != 0) k++;\n",
        "\t\tnum_blocks /= k;\n",
        "\t\tnum_kernels *= k;\n",
        "\t}\n",
        "\tthreads_per_kernel = num_blocks * num_threads;\n",
        "\tprintf(\"K selected: %d\\n\", selected_K);\n",
        "\tprintf(\"num_threads %llu thread_perms %llu num_blocks %llu num_kernels %llu threads_per_kernel %llu\\n\", num_threads, thread_perms, num_blocks, num_kernels, threads_per_kernel);\n",
        "\n",
        "\tdim3 block_dim(num_threads, 1, 1);\n",
        "\tdim3 grid_dim(num_blocks, 1, 1);\n",
        "  SAFE(city_ids = (int8_t *)malloc(size * size8));\n",
        "\tSAFE(shortestPath = (int8_t *)calloc(num_blocks * size, size8));\n",
        "\tSAFE(graphWeights = (int8_t *)malloc(size * size8 * size));\n",
        "\tSAFE(cost = (int32_t *)calloc(num_blocks * size, size32));\n",
        "\tSAFE(choices = (int8_t *)malloc(threads_per_kernel * size * size8));\n",
        "\n",
        "  CUDA_RUN(cudaMalloc((void **)&dev_city_ids, size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_shortestPath, size * size8 * num_blocks));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_graphWeights, size * size8 * size));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_cost, num_blocks * size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_size, size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_selected_K, size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_choices, threads_per_kernel * size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_threads_per_kernel, sizeof(unsigned long long)));\n",
        "\n",
        "  srand(time(NULL));\n",
        "\tinitialize(city_ids, graphWeights, size);\n",
        "\n",
        "\tCUDA_RUN(cudaMemcpy(dev_city_ids, city_ids, size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_shortestPath, shortestPath, size * size8 * num_blocks, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_graphWeights, graphWeights, size * size8 * size, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_size, &size, size32, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_selected_K, &selected_K, size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_choices, choices, threads_per_kernel * size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_threads_per_kernel, &threads_per_kernel, sizeof(unsigned long long), cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_cost, cost, num_blocks * size32, cudaMemcpyHostToDevice));\n",
        "\n",
        "\tCUDA_RUN(cudaMemcpy(dev_city_ids, city_ids, size * size8, cudaMemcpyHostToDevice));\n",
        "  CUDA_RUN(cudaMemcpy(dev_shortestPath, shortestPath, size * size8 * num_blocks, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_graphWeights, graphWeights, size * size8 * size, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_size, &size, size32, cudaMemcpyHostToDevice));\n",
        "  CUDA_RUN(cudaMemcpy(dev_selected_K, &selected_K, size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_choices, choices, threads_per_kernel * size * size8, cudaMemcpyHostToDevice));\n",
        "  CUDA_RUN(cudaMemcpy(dev_threads_per_kernel, &threads_per_kernel, sizeof(unsigned long long), cudaMemcpyHostToDevice));\n",
        "  CUDA_RUN(cudaMemcpy(dev_cost, cost, num_blocks * size32, cudaMemcpyHostToDevice));\n",
        "\n",
        "\tfloat percentage;\n",
        "\tfor (int i = 0; i < num_kernels; i++) {\n",
        "\t\tfind_permutations_for_threads << < 1, 1 >> >(dev_city_ids, dev_selected_K, dev_choices, dev_size, dev_threads_per_kernel);\n",
        "    CUDA_RUN(cudaGetLastError());\n",
        "\t\tCUDA_RUN(cudaDeviceSynchronize());\n",
        "    combinations_kernel << < grid_dim, block_dim >> > (dev_choices, dev_selected_K, dev_shortestPath, dev_graphWeights, dev_cost, dev_size);\n",
        "\t\tCUDA_RUN(cudaGetLastError());\n",
        "\t\tCUDA_RUN(cudaDeviceSynchronize());\n",
        "    percentage = (100. / (float) num_kernels * (float)(i + 1));\n",
        "\t\tprintf(\"\\rProgress : \");\n",
        "\t\tfor (int j = 0; j < 10; j++) {\n",
        "\t\t\tif ((percentage / 10) / j > 1) printf(\"#\");\n",
        "\t\t\telse printf(\" \");\n",
        "\t\t}\n",
        "\t\tprintf(\" [%.2f%%]\", percentage);\n",
        "\t\tfflush(stdout);\n",
        "\t}\n",
        "  CUDA_RUN(cudaMemcpy(shortestPath, dev_shortestPath, num_blocks * size * size8, cudaMemcpyDeviceToHost));\n",
        "\tCUDA_RUN(cudaMemcpy(cost, dev_cost, num_blocks * size32, cudaMemcpyDeviceToHost));\n",
        "\n",
        "\tprint_Graph(graphWeights, size);\n",
        "\n",
        "\t{\n",
        "\t\tint32_t min = cost[0];\n",
        "\t\tint8_t index = 0;\n",
        "\t\tfor (int i = 1; i < num_blocks; i++) {\n",
        "\t\t\tif (cost[i] < min) {\n",
        "\t\t\t\tmin = cost[i];\n",
        "\t\t\t\tindex = i;\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\tprintf(\"Shortest path found on block #%d:\\n\", index + 1);\n",
        "\t\tprint_ShortestPath(&shortestPath[index * size], min, size);\n",
        "\t}\n",
        "\n",
        "\n",
        "  Error:\n",
        "\tfree(city_ids);\n",
        "\tfree(shortestPath);\n",
        "\tfree(graphWeights);\n",
        "\tfree(cost);\n",
        "\tfree(choices);\n",
        "\n",
        "\tcudaFree(dev_city_ids);\n",
        "\tcudaFree(dev_shortestPath);\n",
        "\tcudaFree(dev_graphWeights);\n",
        "\tcudaFree(dev_cost);\n",
        "\tcudaFree(dev_size);\n",
        "\tcudaFree(dev_selected_K);\n",
        "\tcudaFree(dev_choices);\n",
        "\tcudaFree(dev_threads_per_kernel);\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AjgY_rGKuh-",
        "outputId": "5cc65b65-e45b-4975-ca06-a5bbfc4b2921"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "factorial(4): 24\n",
            "K selected: 1\n",
            "num_threads 4 thread_perms 6 num_blocks 1 num_kernels 1 threads_per_kernel 4\n",
            "Initializing the problem\n",
            "0,\t81,\t40,\t82,\t1,\t\n",
            "1,\t0,\t50,\t60,\t79,\t\n",
            "44,\t1,\t0,\t18,\t16,\t\n",
            "98,\t20,\t1,\t0,\t53,\t\n",
            "57,\t31,\t69,\t1,\t0,\t\n",
            "In the kernel\rProgress : ########## [100.00%]0\t81\t40\t82\t1\t\n",
            "1\t0\t50\t60\t79\t\n",
            "44\t1\t0\t18\t16\t\n",
            "98\t20\t1\t0\t53\t\n",
            "57\t31\t69\t1\t0\t\n",
            "Shortest path found on block #1:\n",
            "0\t4\t3\t2\t1\t\n",
            "Cost: 5\n",
            "\n"
          ]
        }
      ]
    }
  ]
}